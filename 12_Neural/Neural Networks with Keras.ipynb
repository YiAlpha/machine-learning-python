{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Install tensorflow and keras libraries first.  Code in command prompt:\n",
    "##     conda install -c conda-forge tensorflow, keras\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "\n",
    "# The core data structure of Keras is a model, a way to organize layers.\n",
    "\n",
    "model = Sequential() # Define the architecture of you model using Sequential.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build layers with Dense, followed by Activation()...\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# one hidden layer with 32 nodes\n",
    "# Activation is set to relu\n",
    "# one output layer with 10 categories.  \n",
    "# softmax function used to calculate 0 to 1 probabilities for each of 10 categories\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model with two hidden layers\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 11,274\n",
      "Trainable params: 11,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Or build a model in steps using .add():\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Once your model looks good, configure its learning process with .compile():\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loss can be set to:**\n",
    "    - 'categorical_crossentropy' for multiple categories\n",
    "    - 'binary_crossentropy' for binary categories\n",
    "    - 'mse' for regression, which calculates the mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer can be set to 'sgd' for stochastic gradient descent or a variety of other techniques.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a keras model\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.7158 - acc: 0.4850 - val_loss: 0.7080 - val_acc: 0.4800\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.7099 - acc: 0.4938 - val_loss: 0.7048 - val_acc: 0.4850\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.7084 - acc: 0.4962 - val_loss: 0.7042 - val_acc: 0.4800\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.7069 - acc: 0.4887 - val_loss: 0.7025 - val_acc: 0.4750\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.7059 - acc: 0.4975 - val_loss: 0.7034 - val_acc: 0.4850\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.7047 - acc: 0.5025 - val_loss: 0.7031 - val_acc: 0.4850\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.7041 - acc: 0.4925 - val_loss: 0.7036 - val_acc: 0.4750\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.7028 - acc: 0.5112 - val_loss: 0.7032 - val_acc: 0.4800\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.7017 - acc: 0.5150 - val_loss: 0.7022 - val_acc: 0.4850\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.7011 - acc: 0.4987 - val_loss: 0.7032 - val_acc: 0.4550\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.7002 - acc: 0.5188 - val_loss: 0.7024 - val_acc: 0.4700\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6994 - acc: 0.5112 - val_loss: 0.7021 - val_acc: 0.4800\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6985 - acc: 0.5212 - val_loss: 0.7028 - val_acc: 0.4550\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6977 - acc: 0.5212 - val_loss: 0.7025 - val_acc: 0.4600\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6968 - acc: 0.5212 - val_loss: 0.7032 - val_acc: 0.4450\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6963 - acc: 0.5212 - val_loss: 0.7020 - val_acc: 0.4600\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6956 - acc: 0.5150 - val_loss: 0.7024 - val_acc: 0.4450\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6951 - acc: 0.5237 - val_loss: 0.7020 - val_acc: 0.4550\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6941 - acc: 0.5337 - val_loss: 0.7013 - val_acc: 0.4650\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6937 - acc: 0.5312 - val_loss: 0.7017 - val_acc: 0.4500\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6927 - acc: 0.5388 - val_loss: 0.7020 - val_acc: 0.4450\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6920 - acc: 0.5437 - val_loss: 0.7023 - val_acc: 0.4600\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6915 - acc: 0.5375 - val_loss: 0.7016 - val_acc: 0.4550\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.6909 - acc: 0.5525 - val_loss: 0.7011 - val_acc: 0.4750\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6901 - acc: 0.5525 - val_loss: 0.7007 - val_acc: 0.4800\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6896 - acc: 0.5487 - val_loss: 0.7008 - val_acc: 0.4800\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.6893 - acc: 0.5587 - val_loss: 0.7004 - val_acc: 0.4750\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6885 - acc: 0.5475 - val_loss: 0.7013 - val_acc: 0.4750\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6880 - acc: 0.5587 - val_loss: 0.7010 - val_acc: 0.4700\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.6872 - acc: 0.5687 - val_loss: 0.7004 - val_acc: 0.4800\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6865 - acc: 0.5725 - val_loss: 0.6995 - val_acc: 0.4850\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6864 - acc: 0.5712 - val_loss: 0.6995 - val_acc: 0.4850\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.6858 - acc: 0.5712 - val_loss: 0.6998 - val_acc: 0.4900\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6852 - acc: 0.5763 - val_loss: 0.6991 - val_acc: 0.5100\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6845 - acc: 0.5800 - val_loss: 0.6990 - val_acc: 0.5100\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6841 - acc: 0.5787 - val_loss: 0.6986 - val_acc: 0.5100\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6837 - acc: 0.5775 - val_loss: 0.6988 - val_acc: 0.5100\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.6830 - acc: 0.5787 - val_loss: 0.6989 - val_acc: 0.5050\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6825 - acc: 0.5813 - val_loss: 0.6985 - val_acc: 0.5100\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.6823 - acc: 0.5962 - val_loss: 0.6981 - val_acc: 0.5200\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.6817 - acc: 0.5775 - val_loss: 0.6983 - val_acc: 0.5100\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6812 - acc: 0.5913 - val_loss: 0.6971 - val_acc: 0.5300\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6810 - acc: 0.5862 - val_loss: 0.6976 - val_acc: 0.5250\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6801 - acc: 0.5925 - val_loss: 0.6972 - val_acc: 0.5300\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6794 - acc: 0.5962 - val_loss: 0.6958 - val_acc: 0.5250\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6791 - acc: 0.5900 - val_loss: 0.6965 - val_acc: 0.5300\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6788 - acc: 0.5988 - val_loss: 0.6964 - val_acc: 0.5300\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6785 - acc: 0.6075 - val_loss: 0.6962 - val_acc: 0.5300\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6782 - acc: 0.5950 - val_loss: 0.6958 - val_acc: 0.5300\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6772 - acc: 0.5975 - val_loss: 0.6956 - val_acc: 0.5350\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6771 - acc: 0.5962 - val_loss: 0.6955 - val_acc: 0.5350\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6763 - acc: 0.6013 - val_loss: 0.6950 - val_acc: 0.5300\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6761 - acc: 0.5887 - val_loss: 0.6956 - val_acc: 0.5500\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6761 - acc: 0.6025 - val_loss: 0.6951 - val_acc: 0.5400\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6752 - acc: 0.6013 - val_loss: 0.6952 - val_acc: 0.5400\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6748 - acc: 0.6000 - val_loss: 0.6951 - val_acc: 0.5400\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6950 - val_acc: 0.5450\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6742 - acc: 0.5988 - val_loss: 0.6946 - val_acc: 0.5450\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.6669 - acc: 0.625 - 0s 35us/step - loss: 0.6736 - acc: 0.6025 - val_loss: 0.6947 - val_acc: 0.5450\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6732 - acc: 0.6187 - val_loss: 0.6955 - val_acc: 0.5450\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6727 - acc: 0.6000 - val_loss: 0.6948 - val_acc: 0.5450\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6724 - acc: 0.6175 - val_loss: 0.6960 - val_acc: 0.5400\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6720 - acc: 0.6150 - val_loss: 0.6938 - val_acc: 0.5450\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.6716 - acc: 0.6100 - val_loss: 0.6945 - val_acc: 0.5550\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6713 - acc: 0.6038 - val_loss: 0.6944 - val_acc: 0.5500\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.6711 - acc: 0.6013 - val_loss: 0.6936 - val_acc: 0.5450\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6708 - acc: 0.6050 - val_loss: 0.6935 - val_acc: 0.5400\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6702 - acc: 0.6187 - val_loss: 0.6941 - val_acc: 0.5500\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6695 - acc: 0.6038 - val_loss: 0.6931 - val_acc: 0.5400\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6697 - acc: 0.6138 - val_loss: 0.6944 - val_acc: 0.5450\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6691 - acc: 0.6125 - val_loss: 0.6935 - val_acc: 0.5450\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6686 - acc: 0.6088 - val_loss: 0.6943 - val_acc: 0.5400\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.6679 - acc: 0.6163 - val_loss: 0.6935 - val_acc: 0.5400\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6677 - acc: 0.6200 - val_loss: 0.6945 - val_acc: 0.5400\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6672 - acc: 0.6038 - val_loss: 0.6938 - val_acc: 0.5300\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6670 - acc: 0.6200 - val_loss: 0.6941 - val_acc: 0.5450\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6662 - acc: 0.6138 - val_loss: 0.6934 - val_acc: 0.5300\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6660 - acc: 0.6212 - val_loss: 0.6937 - val_acc: 0.5300\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6659 - acc: 0.6150 - val_loss: 0.6928 - val_acc: 0.5400\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6653 - acc: 0.6163 - val_loss: 0.6936 - val_acc: 0.5350\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6646 - acc: 0.6212 - val_loss: 0.6940 - val_acc: 0.5450\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.6646 - acc: 0.6125 - val_loss: 0.6932 - val_acc: 0.5300\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.6638 - acc: 0.6262 - val_loss: 0.6945 - val_acc: 0.5600\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6638 - acc: 0.6187 - val_loss: 0.6933 - val_acc: 0.5300\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6630 - acc: 0.6225 - val_loss: 0.6934 - val_acc: 0.5300\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6626 - acc: 0.6187 - val_loss: 0.6946 - val_acc: 0.5600\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6622 - acc: 0.6125 - val_loss: 0.6932 - val_acc: 0.5300\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.6615 - acc: 0.6200 - val_loss: 0.6931 - val_acc: 0.5300\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.6613 - acc: 0.6200 - val_loss: 0.6919 - val_acc: 0.5200\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6612 - acc: 0.6262 - val_loss: 0.6933 - val_acc: 0.5300\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.6603 - acc: 0.6313 - val_loss: 0.6948 - val_acc: 0.5550\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6602 - acc: 0.6187 - val_loss: 0.6934 - val_acc: 0.5250\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6594 - acc: 0.6288 - val_loss: 0.6944 - val_acc: 0.5550\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6590 - acc: 0.6300 - val_loss: 0.6931 - val_acc: 0.5300\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.6586 - acc: 0.6313 - val_loss: 0.6942 - val_acc: 0.5450\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6580 - acc: 0.6212 - val_loss: 0.6929 - val_acc: 0.5250\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6575 - acc: 0.6313 - val_loss: 0.6933 - val_acc: 0.5300\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.6572 - acc: 0.6337 - val_loss: 0.6946 - val_acc: 0.5500\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6566 - acc: 0.6337 - val_loss: 0.6948 - val_acc: 0.5500\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6563 - acc: 0.6400 - val_loss: 0.6937 - val_acc: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129c53e48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100)) # X data\n",
    "labels = np.random.randint(2, size=(1000, 1)) # y data\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, validation_split=0.20, epochs=100, batch_size=32)\n",
    "\n",
    "#Note that you can also use train_test_split() with , validation_data=(X_test,y_test) argument from Keras in same manner.\n",
    "##Split data first and then simply train on training data and add test data to this argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiple categories you  need to one hot encode y using to_categorical()\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(32, activation='relu', input_dim=20))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01)  # define a learning rate for optimization\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128) # extract loss and accuracy from test data evaluation\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "[2 0 2 3 3 2 8 8 9 5 5 4 9 5 9 8 2 8 5 4 4 2 2 1 2 1 4 1 2 8 0 4 1 8 8 6 9\n",
      " 1 9 2 9 5 2 1 8 2 5 1 8 8 2 8 3 2 2 2 2 8 2 2 8 0 1 1 0 1 6 4 4 6 2 3 8 2\n",
      " 8 4 9 2 5 2 9 2 5 9 9 8 5 6 5 8 4 3 2 2 2 5 0 4 4 2]\n",
      "[[0.09530438 0.12640128 0.14621726 0.09291259 0.11307466 0.11318072\n",
      "  0.07685441 0.05781348 0.11308313 0.06515814]\n",
      " [0.12437348 0.11170395 0.09388758 0.09216025 0.12397912 0.08263753\n",
      "  0.09436309 0.086629   0.09796672 0.09229928]\n",
      " [0.09669864 0.11523381 0.12813061 0.09071911 0.08688845 0.06040128\n",
      "  0.10969058 0.0661376  0.1240026  0.12209725]\n",
      " [0.10957269 0.08833002 0.08764888 0.13662116 0.09580473 0.09058928\n",
      "  0.09518266 0.0795687  0.12723008 0.08945177]\n",
      " [0.08436732 0.0941147  0.11326867 0.13554572 0.07510682 0.10752569\n",
      "  0.08988754 0.09942142 0.13360652 0.06715555]\n",
      " [0.09743612 0.09647243 0.1295585  0.08651644 0.09976729 0.11611336\n",
      "  0.12491124 0.12085231 0.06286434 0.06550796]\n",
      " [0.09929967 0.10475828 0.10140976 0.07904141 0.10561639 0.093706\n",
      "  0.07539123 0.10495515 0.12321106 0.11261105]\n",
      " [0.09100439 0.09387695 0.09952012 0.11633181 0.09584526 0.11775053\n",
      "  0.07924037 0.08716972 0.13589951 0.08336137]\n",
      " [0.10026117 0.10513119 0.11834463 0.0668697  0.09077781 0.0881477\n",
      "  0.10762186 0.08337392 0.09577853 0.14369352]\n",
      " [0.0864103  0.09739285 0.12067267 0.08089102 0.08992477 0.12619539\n",
      "  0.07912962 0.09139373 0.11934744 0.10864226]\n",
      " [0.08661114 0.08154397 0.12366203 0.09114058 0.10818584 0.13574502\n",
      "  0.09727025 0.094223   0.09951058 0.08210762]\n",
      " [0.10107008 0.07605751 0.08447714 0.11916749 0.12540472 0.12170193\n",
      "  0.11258186 0.09103839 0.10391479 0.06458608]\n",
      " [0.09958683 0.09380588 0.11508762 0.0740301  0.10315043 0.11513332\n",
      "  0.09215508 0.08790123 0.09850644 0.12064309]\n",
      " [0.09575055 0.08244768 0.10958546 0.10660753 0.11959755 0.125123\n",
      "  0.09508887 0.11287338 0.09710644 0.05581959]\n",
      " [0.09366652 0.06715038 0.100164   0.07969122 0.11004303 0.1292523\n",
      "  0.09729675 0.08935344 0.1016186  0.13176377]\n",
      " [0.09617697 0.08753367 0.09897886 0.08817225 0.10358971 0.10393007\n",
      "  0.09224011 0.10385925 0.11753403 0.10798505]\n",
      " [0.0873788  0.11884988 0.13816541 0.08619457 0.13603541 0.08628432\n",
      "  0.10326767 0.06811024 0.1088666  0.06684715]\n",
      " [0.10347791 0.09709629 0.10005972 0.0912312  0.11815014 0.09426355\n",
      "  0.08124711 0.09403487 0.13040027 0.09003898]\n",
      " [0.108047   0.07577956 0.08518872 0.07646021 0.13272002 0.14778464\n",
      "  0.1202604  0.10113869 0.06067104 0.09194968]\n",
      " [0.10799512 0.09861524 0.09019551 0.11336132 0.13350673 0.11158482\n",
      "  0.12009388 0.07259665 0.09128509 0.06076559]\n",
      " [0.10336105 0.10398472 0.12732892 0.06984441 0.132392   0.10606518\n",
      "  0.08878185 0.08479171 0.10228318 0.08116703]\n",
      " [0.10161117 0.10151584 0.13190457 0.06843399 0.09803594 0.11061694\n",
      "  0.09112805 0.09369106 0.08443161 0.11863089]\n",
      " [0.10414946 0.07906867 0.13933921 0.06820798 0.09051154 0.12022203\n",
      "  0.10584257 0.09519191 0.08045786 0.11700878]\n",
      " [0.09840368 0.110333   0.10890561 0.10112711 0.09634718 0.10901253\n",
      "  0.09500244 0.10185508 0.097962   0.08105137]\n",
      " [0.09875909 0.12352204 0.14859208 0.07299364 0.08998842 0.08343481\n",
      "  0.08984253 0.08997395 0.1090886  0.09380492]\n",
      " [0.10248966 0.12615235 0.10931394 0.09436236 0.09643494 0.08591876\n",
      "  0.08299538 0.08854052 0.11093206 0.10286   ]\n",
      " [0.10338967 0.10060535 0.09047308 0.08873767 0.11907634 0.10613357\n",
      "  0.08727264 0.09935854 0.10817442 0.0967787 ]\n",
      " [0.11163583 0.140364   0.1358432  0.08429307 0.10838283 0.08066227\n",
      "  0.06608123 0.07957213 0.10158548 0.09157996]\n",
      " [0.11481991 0.09707392 0.119542   0.08267666 0.10778405 0.0973208\n",
      "  0.10076922 0.08095364 0.10001874 0.09904101]\n",
      " [0.10412341 0.08591202 0.11328511 0.09665877 0.09951562 0.10464334\n",
      "  0.07938739 0.09592756 0.13260882 0.08793795]\n",
      " [0.11152013 0.07835796 0.09269223 0.10180216 0.09956452 0.10600508\n",
      "  0.11076514 0.09810336 0.10661061 0.09457877]\n",
      " [0.11296283 0.10777223 0.11234419 0.0799994  0.12173086 0.09647234\n",
      "  0.10386445 0.07935919 0.08592036 0.0995741 ]\n",
      " [0.11791942 0.13552225 0.11192136 0.08925863 0.10912401 0.07377978\n",
      "  0.07661483 0.07047621 0.11350266 0.10188089]\n",
      " [0.10819172 0.10499371 0.10329361 0.11978738 0.10089579 0.07385003\n",
      "  0.10244413 0.07749808 0.12337644 0.08566916]\n",
      " [0.09529784 0.09747328 0.10507182 0.08981607 0.10290394 0.10566521\n",
      "  0.08591362 0.10551322 0.11558571 0.09675937]\n",
      " [0.0977846  0.08156732 0.10202862 0.08540605 0.10659391 0.12952602\n",
      "  0.130036   0.10453001 0.07233798 0.09018947]\n",
      " [0.10734502 0.11158523 0.11584374 0.05444337 0.11230367 0.10183582\n",
      "  0.09971131 0.07481132 0.08329281 0.1388277 ]\n",
      " [0.11248916 0.13611124 0.10006565 0.09920365 0.10588975 0.08691451\n",
      "  0.08512649 0.09100756 0.11125694 0.07193496]\n",
      " [0.11966531 0.0914916  0.10891069 0.07496276 0.11829322 0.09862769\n",
      "  0.10794093 0.06938194 0.0788612  0.13186479]\n",
      " [0.08662673 0.11133049 0.1325899  0.08485703 0.09844286 0.11442626\n",
      "  0.08674876 0.10942259 0.10311319 0.07244213]\n",
      " [0.10740416 0.09374051 0.09838443 0.08063914 0.10973216 0.09902319\n",
      "  0.1053877  0.08928245 0.09740227 0.11900412]\n",
      " [0.10335594 0.07547361 0.12919985 0.07493709 0.10480204 0.13821521\n",
      "  0.08343058 0.09245326 0.09371509 0.1044173 ]\n",
      " [0.09775791 0.09331304 0.16500174 0.07307901 0.09490634 0.09522241\n",
      "  0.11562056 0.11074858 0.08299163 0.07135876]\n",
      " [0.08416101 0.12071697 0.11221531 0.09251515 0.10077372 0.10806235\n",
      "  0.0867871  0.09010632 0.11488263 0.08977949]\n",
      " [0.10145143 0.12095145 0.11624815 0.09739892 0.09716788 0.07612678\n",
      "  0.0905774  0.09810657 0.1263898  0.07558168]\n",
      " [0.09901027 0.09956272 0.1345641  0.07255951 0.09193918 0.11440689\n",
      "  0.09356584 0.09281664 0.09610099 0.10547391]\n",
      " [0.10159697 0.09263293 0.10027238 0.10380622 0.08344579 0.11239954\n",
      "  0.09570705 0.10164098 0.11201927 0.09647886]\n",
      " [0.09532678 0.12802008 0.12170334 0.07628898 0.09955736 0.08255076\n",
      "  0.08906115 0.0927771  0.11254754 0.10216695]\n",
      " [0.10775395 0.10937938 0.08118404 0.11608784 0.118086   0.08626273\n",
      "  0.08796334 0.10766588 0.12307043 0.06254645]\n",
      " [0.0905179  0.10236671 0.11063486 0.13977104 0.09561026 0.10367114\n",
      "  0.07282843 0.08427745 0.14668933 0.05363286]\n",
      " [0.09296498 0.11855685 0.1334379  0.06987638 0.08717624 0.10097221\n",
      "  0.11196198 0.10793506 0.0840987  0.09301966]\n",
      " [0.10829155 0.09903884 0.10400888 0.08604928 0.11964969 0.10311101\n",
      "  0.07275446 0.07347772 0.13078947 0.10282904]\n",
      " [0.10995153 0.08368899 0.10663875 0.1254774  0.09453587 0.11648616\n",
      "  0.09392062 0.08779643 0.11208635 0.06941787]\n",
      " [0.09311676 0.07711069 0.12889473 0.09197161 0.12158207 0.12423697\n",
      "  0.10256908 0.12225226 0.07927853 0.05898729]\n",
      " [0.0892515  0.11096063 0.12234571 0.05573146 0.11600163 0.09357778\n",
      "  0.08851757 0.09774752 0.10973802 0.11612823]\n",
      " [0.10906353 0.12540363 0.1296338  0.07958785 0.09559803 0.08803139\n",
      "  0.07311012 0.08770136 0.11347874 0.09839154]\n",
      " [0.10414737 0.10457473 0.14554605 0.07725517 0.08091152 0.07813869\n",
      "  0.12212875 0.08905866 0.09103066 0.10720848]\n",
      " [0.10030369 0.085329   0.10253333 0.10034789 0.08889461 0.10437702\n",
      "  0.09502774 0.08556921 0.12611265 0.11150484]\n",
      " [0.09256031 0.09445889 0.13123849 0.07904756 0.09958459 0.122057\n",
      "  0.08374884 0.11493314 0.09141657 0.09095461]\n",
      " [0.10433677 0.10678517 0.13428491 0.05444369 0.10719053 0.10037398\n",
      "  0.12403809 0.09713832 0.05763929 0.11376922]\n",
      " [0.10277268 0.1324989  0.11452593 0.1086745  0.10854512 0.08316147\n",
      "  0.06964996 0.07266276 0.133579   0.07392969]\n",
      " [0.11731413 0.10711192 0.11298282 0.08492927 0.11553885 0.08703463\n",
      "  0.09728891 0.09097216 0.10126097 0.08556636]\n",
      " [0.11244877 0.15073769 0.13413164 0.06875163 0.10237841 0.06485683\n",
      "  0.07842924 0.07523943 0.09413356 0.11889281]\n",
      " [0.10699391 0.10925791 0.07978696 0.10479548 0.10861231 0.10535429\n",
      "  0.09264985 0.10799258 0.09692512 0.08763157]\n",
      " [0.11960831 0.10287094 0.11023204 0.07897267 0.11229508 0.08193846\n",
      "  0.11858407 0.09210789 0.08148359 0.10190696]\n",
      " [0.10220855 0.14179792 0.12592803 0.06209701 0.12781726 0.08272653\n",
      "  0.0640171  0.09707065 0.10601322 0.09032381]\n",
      " [0.11704241 0.08833133 0.07926191 0.09060458 0.12168913 0.10918187\n",
      "  0.13558662 0.10188972 0.07081769 0.08559465]\n",
      " [0.11829789 0.12389839 0.11466877 0.06054035 0.12903748 0.08819409\n",
      "  0.10686383 0.08039293 0.06410442 0.11400183]\n",
      " [0.09770251 0.1156195  0.09872572 0.08775963 0.1329835  0.10208566\n",
      "  0.11512031 0.08781873 0.09956091 0.06262351]\n",
      " [0.09823165 0.11037526 0.10347792 0.10269924 0.1026255  0.10300798\n",
      "  0.11779222 0.10940108 0.09169861 0.06069051]\n",
      " [0.0983152  0.12406731 0.12885517 0.05916385 0.09971919 0.09540379\n",
      "  0.10380971 0.10082135 0.07694244 0.11290197]\n",
      " [0.12394202 0.09118588 0.07789844 0.13148907 0.10903356 0.09059544\n",
      "  0.1056319  0.08897702 0.11627456 0.06497214]\n",
      " [0.10929459 0.08038937 0.08245411 0.12218663 0.10260924 0.1052639\n",
      "  0.08542536 0.09557658 0.13215326 0.08464694]\n",
      " [0.11054516 0.0823906  0.13101864 0.09286153 0.09829726 0.10543444\n",
      "  0.1277157  0.1067673  0.07966211 0.06530718]\n",
      " [0.09379065 0.10260144 0.11100145 0.0975069  0.09493184 0.11437665\n",
      "  0.06516738 0.10082152 0.11957364 0.10022847]\n",
      " [0.11208034 0.10249986 0.1079198  0.07253929 0.12479541 0.1102054\n",
      "  0.11415149 0.09175455 0.07253924 0.09151459]\n",
      " [0.09169252 0.11138865 0.1123515  0.06238162 0.10636161 0.1009538\n",
      "  0.0707283  0.06698436 0.10799578 0.16916183]\n",
      " [0.10106067 0.10614221 0.14433728 0.08327405 0.11740009 0.10147278\n",
      "  0.09116594 0.06499346 0.09778283 0.09237069]\n",
      " [0.09660298 0.10606653 0.10622737 0.09665391 0.0898321  0.11113758\n",
      "  0.10292426 0.09607021 0.10912967 0.08535548]\n",
      " [0.10179672 0.09585864 0.11851662 0.10200714 0.10359263 0.11573976\n",
      "  0.09510297 0.08799925 0.10927142 0.07011478]\n",
      " [0.09441224 0.09325138 0.10463215 0.07094671 0.11274002 0.10654092\n",
      "  0.10955644 0.09863148 0.09071547 0.11857324]\n",
      " [0.09742731 0.11527104 0.14521922 0.06251831 0.09681746 0.08700649\n",
      "  0.09082428 0.10082611 0.09468593 0.10940391]\n",
      " [0.08344166 0.0850203  0.12766524 0.07734866 0.09647344 0.13610563\n",
      "  0.12290217 0.10592628 0.08164528 0.08347137]\n",
      " [0.0854203  0.09646549 0.11160068 0.06962646 0.11105614 0.11524569\n",
      "  0.07255222 0.09248371 0.1218904  0.12365887]\n",
      " [0.10544712 0.10023022 0.10713381 0.07439395 0.09978085 0.10156281\n",
      "  0.09208814 0.07164406 0.10386272 0.14385638]\n",
      " [0.10904593 0.10675999 0.10130057 0.11606929 0.08894444 0.09001677\n",
      "  0.09836234 0.08350012 0.12238816 0.08361244]\n",
      " [0.10142422 0.09142061 0.09984478 0.07797277 0.11950322 0.13156316\n",
      "  0.11311924 0.09519587 0.08235675 0.08759936]\n",
      " [0.10615034 0.08774979 0.08768614 0.09820537 0.09661766 0.10988287\n",
      "  0.11498116 0.10504851 0.10548969 0.08818842]\n",
      " [0.0979479  0.09504065 0.10593759 0.08974031 0.12412168 0.12569708\n",
      "  0.08593591 0.07759228 0.11698736 0.0809992 ]\n",
      " [0.10856366 0.09535473 0.11102521 0.10651278 0.09360754 0.08400757\n",
      "  0.102051   0.10413835 0.11440613 0.080333  ]\n",
      " [0.10398533 0.1062619  0.11283392 0.0870169  0.11846073 0.10567893\n",
      "  0.08978935 0.09444407 0.09574838 0.08578049]\n",
      " [0.09276283 0.09733362 0.09607085 0.12730268 0.08025605 0.10539864\n",
      "  0.10082123 0.10174228 0.12372472 0.07458705]\n",
      " [0.07830817 0.09092704 0.15236582 0.05756859 0.09539983 0.11950845\n",
      "  0.08874907 0.08900726 0.09940186 0.1287639 ]\n",
      " [0.08437689 0.10727084 0.14912859 0.06987354 0.09919013 0.08517677\n",
      "  0.12697342 0.07916607 0.0863093  0.11253446]\n",
      " [0.11101618 0.09403522 0.1459139  0.06701069 0.09374736 0.08302291\n",
      "  0.09825592 0.0868423  0.09372224 0.12643327]\n",
      " [0.11488557 0.08366176 0.08561048 0.09670778 0.10608032 0.12656978\n",
      "  0.11928482 0.09281178 0.08010156 0.09428631]\n",
      " [0.1224855  0.10914652 0.09809534 0.10149555 0.11679441 0.08226672\n",
      "  0.09936045 0.08913931 0.10329352 0.07792264]\n",
      " [0.10486928 0.08092181 0.08826281 0.10592908 0.12520038 0.12087351\n",
      "  0.10645652 0.10600621 0.09513597 0.06634454]\n",
      " [0.09996597 0.10025975 0.11496659 0.08366187 0.12493449 0.11150496\n",
      "  0.08054926 0.08774377 0.09922019 0.09719322]\n",
      " [0.10263646 0.11719687 0.14397675 0.08236153 0.09819055 0.08713856\n",
      "  0.08719882 0.07669764 0.10024778 0.10435498]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prediction from keras classification model\n",
    "print(x_test.shape)\n",
    "\n",
    "# for predicted probabilities and labels\n",
    "ypreds = model.predict_classes(x_test)\n",
    "print(ypreds)\n",
    "\n",
    "# for predicted probabilities\n",
    "ypreds = model.predict_proba(x_test)\n",
    "print(ypreds) #gives prediction of each category, largest is selected for predict_classes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction from keras regression model\n",
    "\n",
    "# for predicted probabilities and labels\n",
    "ypreds = model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict keras model with sklearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332, 5)\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# Use KerasRegressor for regression model tuning\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pimas diabetes dataset\n",
    "dataset = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Pima.te.csv\", delimiter=\",\")\n",
    "\n",
    "X = dataset.iloc[:,1:6]\n",
    "Y = dataset.iloc[:,7]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple example\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=5, activation='relu'))\n",
    "\tmodel.add(Dense(8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, verbose=0) # epochs arg is built in to Scikit learn's... \n",
    "                                                                      # KerasClassifier\n",
    "\n",
    "# Building a simple search grid that adjusts epochs\n",
    "param_grid = dict(epochs=[10,20,30])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.123494 using {'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# grid_result.cv_results_ for full results file\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.123494 using {'learn_rate': 0.01}\n",
      "0.102410 (0.028191) with: {'learn_rate': 0.001}\n",
      "0.123494 (0.004543) with: {'learn_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create function that builds model\n",
    "# Function to create model, required for KerasClassifier\n",
    "\n",
    "#In order to tune parameters native to keras, add them as arguments to your create_model function\n",
    "\n",
    "def create_model(learn_rate=0.01):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=5, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\toptimizer = SGD(lr=learn_rate)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "#call model function in KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate) # set dictionary using function this time\n",
    "\n",
    "#Using n_jobs=-1 to parallelize across available processors to speed it up\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you try.  Can you fit a neural network model to the Iris dataset?  Run models that change the structure of the network (i.e.-hidden layers and activations).  Try to improve your validation accuracy as much as possible.\n",
    "\n",
    "Data can be imported via the following link:\n",
    "\n",
    "http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0          setosa\n",
       "10         setosa\n",
       "20         setosa\n",
       "30         setosa\n",
       "40         setosa\n",
       "50     versicolor\n",
       "60     versicolor\n",
       "70     versicolor\n",
       "80     versicolor\n",
       "90     versicolor\n",
       "100     virginica\n",
       "110     virginica\n",
       "120     virginica\n",
       "130     virginica\n",
       "140     virginica\n",
       "Name: Species, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# target = InMichelin, whether or not a restaurant is in the Michelin guide\n",
    "data = pd.read_csv(\"http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\" , encoding=\"latin_1\")\n",
    "\n",
    "\n",
    "#update data to set up for train test split\n",
    "data = data.iloc[:,1:]\n",
    "y = data['Species']\n",
    "X = data.loc[:, data.columns != 'Species']\n",
    "\n",
    "display(data.head())\n",
    "display(X.head())\n",
    "display(y[0::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

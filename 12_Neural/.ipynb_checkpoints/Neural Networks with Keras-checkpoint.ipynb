{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Parrott\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Michael Parrott\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# Install tensorflow and keras libraries first.  Code in command prompt:\n",
    "##     conda install -c conda-forge tensorflow, keras\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "\n",
    "# The core data structure of Keras is a model, a way to organize layers.\n",
    "\n",
    "model = Sequential() # Define the architecture of you model using Sequential.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build layers with Dense, followed by Activation()...\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# one hidden layer with 32 nodes\n",
    "# Activation is set to relu\n",
    "# one output layer with 10 categories.  \n",
    "# softmax function used to calculate 0 to 1 probabilities for each of 10 categories\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model with two hidden layers\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 11,274\n",
      "Trainable params: 11,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Or build a model in steps using .add():\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Once your model looks good, configure its learning process with .compile():\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loss can be set to:**\n",
    "    - 'categorical_crossentropy' for multiple categories\n",
    "    - 'binary_crossentropy' for binary categories\n",
    "    - 'mse' for regression, which calculates the mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer can be set to 'sgd' for stochastic gradient descent or a variety of other techniques.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a keras model\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.7078 - acc: 0.5150 - val_loss: 0.7105 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 256us/step - loss: 0.6999 - acc: 0.5087 - val_loss: 0.7032 - val_acc: 0.5350\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.6975 - acc: 0.5012 - val_loss: 0.6993 - val_acc: 0.5400\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.6967 - acc: 0.5150 - val_loss: 0.7020 - val_acc: 0.5050\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 381us/step - loss: 0.6954 - acc: 0.5125 - val_loss: 0.7012 - val_acc: 0.5250\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 375us/step - loss: 0.6945 - acc: 0.5125 - val_loss: 0.7005 - val_acc: 0.5250\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s 375us/step - loss: 0.6938 - acc: 0.5175 - val_loss: 0.6995 - val_acc: 0.5250\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 263us/step - loss: 0.6927 - acc: 0.5325 - val_loss: 0.6999 - val_acc: 0.5350\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.6920 - acc: 0.5250 - val_loss: 0.7012 - val_acc: 0.5250\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.6909 - acc: 0.5325 - val_loss: 0.7031 - val_acc: 0.5100\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.6902 - acc: 0.5225 - val_loss: 0.7033 - val_acc: 0.5100\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 232us/step - loss: 0.6893 - acc: 0.5237 - val_loss: 0.7019 - val_acc: 0.5350\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 202us/step - loss: 0.6886 - acc: 0.5363 - val_loss: 0.7029 - val_acc: 0.5300\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 178us/step - loss: 0.6877 - acc: 0.5312 - val_loss: 0.7035 - val_acc: 0.5300\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 193us/step - loss: 0.6869 - acc: 0.5350 - val_loss: 0.7036 - val_acc: 0.5150\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 194us/step - loss: 0.6858 - acc: 0.5375 - val_loss: 0.7034 - val_acc: 0.5150\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 232us/step - loss: 0.6847 - acc: 0.5500 - val_loss: 0.7032 - val_acc: 0.5150\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 181us/step - loss: 0.6846 - acc: 0.5475 - val_loss: 0.7037 - val_acc: 0.5100\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 191us/step - loss: 0.6835 - acc: 0.5487 - val_loss: 0.7050 - val_acc: 0.4900\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 206us/step - loss: 0.6829 - acc: 0.5463 - val_loss: 0.7051 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.6823 - acc: 0.5513 - val_loss: 0.7038 - val_acc: 0.5150\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.6813 - acc: 0.5587 - val_loss: 0.7060 - val_acc: 0.5050\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.6811 - acc: 0.5550 - val_loss: 0.7058 - val_acc: 0.5100\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 157us/step - loss: 0.6803 - acc: 0.5487 - val_loss: 0.7045 - val_acc: 0.5100\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 208us/step - loss: 0.6802 - acc: 0.5500 - val_loss: 0.7046 - val_acc: 0.5150\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 178us/step - loss: 0.6790 - acc: 0.5538 - val_loss: 0.7066 - val_acc: 0.5050\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.6785 - acc: 0.5575 - val_loss: 0.7043 - val_acc: 0.5200\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.6778 - acc: 0.5500 - val_loss: 0.7038 - val_acc: 0.5250\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 155us/step - loss: 0.6774 - acc: 0.5625 - val_loss: 0.7038 - val_acc: 0.5250\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 155us/step - loss: 0.6770 - acc: 0.5663 - val_loss: 0.7053 - val_acc: 0.5200\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 170us/step - loss: 0.6763 - acc: 0.5587 - val_loss: 0.7044 - val_acc: 0.5150\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 167us/step - loss: 0.6755 - acc: 0.5712 - val_loss: 0.7038 - val_acc: 0.5250\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6751 - acc: 0.5725 - val_loss: 0.7021 - val_acc: 0.5100\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.6747 - acc: 0.5725 - val_loss: 0.7029 - val_acc: 0.5200\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.6737 - acc: 0.5775 - val_loss: 0.7042 - val_acc: 0.5050\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6736 - acc: 0.5763 - val_loss: 0.7023 - val_acc: 0.5050\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.6728 - acc: 0.5763 - val_loss: 0.7033 - val_acc: 0.5100\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.6719 - acc: 0.5862 - val_loss: 0.7031 - val_acc: 0.5050\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.6711 - acc: 0.5825 - val_loss: 0.7034 - val_acc: 0.5050\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.6706 - acc: 0.5850 - val_loss: 0.7050 - val_acc: 0.5100\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6699 - acc: 0.5825 - val_loss: 0.7058 - val_acc: 0.5100\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6694 - acc: 0.5837 - val_loss: 0.7033 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.6689 - acc: 0.5800 - val_loss: 0.7035 - val_acc: 0.5050\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6680 - acc: 0.5988 - val_loss: 0.7066 - val_acc: 0.5100\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.6676 - acc: 0.5975 - val_loss: 0.7045 - val_acc: 0.5050\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.6671 - acc: 0.5950 - val_loss: 0.7015 - val_acc: 0.5150\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6660 - acc: 0.6000 - val_loss: 0.7068 - val_acc: 0.5150\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.6659 - acc: 0.6013 - val_loss: 0.7042 - val_acc: 0.5000\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6657 - acc: 0.5950 - val_loss: 0.7034 - val_acc: 0.5250\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.6648 - acc: 0.6025 - val_loss: 0.7055 - val_acc: 0.5100\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.6642 - acc: 0.5988 - val_loss: 0.7055 - val_acc: 0.5000\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.6635 - acc: 0.5962 - val_loss: 0.7019 - val_acc: 0.4950\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6626 - acc: 0.6112 - val_loss: 0.7063 - val_acc: 0.5100\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6619 - acc: 0.6050 - val_loss: 0.7052 - val_acc: 0.5050\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6613 - acc: 0.6038 - val_loss: 0.7028 - val_acc: 0.5150\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.6608 - acc: 0.6138 - val_loss: 0.7057 - val_acc: 0.5100\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.6602 - acc: 0.6025 - val_loss: 0.7067 - val_acc: 0.5050\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6591 - acc: 0.6062 - val_loss: 0.7027 - val_acc: 0.5000\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.6595 - acc: 0.6125 - val_loss: 0.7033 - val_acc: 0.5100\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.6583 - acc: 0.6187 - val_loss: 0.7082 - val_acc: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6575 - acc: 0.6163 - val_loss: 0.7044 - val_acc: 0.5100\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6573 - acc: 0.6163 - val_loss: 0.7045 - val_acc: 0.5100\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6566 - acc: 0.6238 - val_loss: 0.7044 - val_acc: 0.5050\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.6561 - acc: 0.6125 - val_loss: 0.7054 - val_acc: 0.5100\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.6560 - acc: 0.6200 - val_loss: 0.7049 - val_acc: 0.5150\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.6550 - acc: 0.6288 - val_loss: 0.7066 - val_acc: 0.5000\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.6539 - acc: 0.6238 - val_loss: 0.7037 - val_acc: 0.4950\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.6541 - acc: 0.6225 - val_loss: 0.7025 - val_acc: 0.5000\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.6526 - acc: 0.6362 - val_loss: 0.7013 - val_acc: 0.5050\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.6527 - acc: 0.6238 - val_loss: 0.7048 - val_acc: 0.5100\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.6517 - acc: 0.6288 - val_loss: 0.7053 - val_acc: 0.5100\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.6512 - acc: 0.6375 - val_loss: 0.7060 - val_acc: 0.5100\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 98us/step - loss: 0.6511 - acc: 0.6262 - val_loss: 0.7088 - val_acc: 0.5000\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 106us/step - loss: 0.6499 - acc: 0.6350 - val_loss: 0.7121 - val_acc: 0.5100\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.6491 - acc: 0.6425 - val_loss: 0.7046 - val_acc: 0.5050\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.6487 - acc: 0.6375 - val_loss: 0.7073 - val_acc: 0.5100\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.6482 - acc: 0.6375 - val_loss: 0.7045 - val_acc: 0.5050\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6480 - acc: 0.6337 - val_loss: 0.7069 - val_acc: 0.5000\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.6467 - acc: 0.6388 - val_loss: 0.7069 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.6462 - acc: 0.6400 - val_loss: 0.7046 - val_acc: 0.5100\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.6458 - acc: 0.6450 - val_loss: 0.7059 - val_acc: 0.4950\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.6456 - acc: 0.6337 - val_loss: 0.7059 - val_acc: 0.5000\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.6449 - acc: 0.6475 - val_loss: 0.7080 - val_acc: 0.4850\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.6436 - acc: 0.6487 - val_loss: 0.7074 - val_acc: 0.4900\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.6435 - acc: 0.6425 - val_loss: 0.7109 - val_acc: 0.5100\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6428 - acc: 0.6487 - val_loss: 0.7056 - val_acc: 0.5050\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 163us/step - loss: 0.6432 - acc: 0.6462 - val_loss: 0.7096 - val_acc: 0.4900\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.6411 - acc: 0.6487 - val_loss: 0.7092 - val_acc: 0.4950\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.6406 - acc: 0.6475 - val_loss: 0.7065 - val_acc: 0.4900\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.6400 - acc: 0.6550 - val_loss: 0.7124 - val_acc: 0.5150\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.6395 - acc: 0.6525 - val_loss: 0.7109 - val_acc: 0.4950\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.6386 - acc: 0.6450 - val_loss: 0.7103 - val_acc: 0.4950\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.6384 - acc: 0.6613 - val_loss: 0.7064 - val_acc: 0.4900\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.6378 - acc: 0.6438 - val_loss: 0.7127 - val_acc: 0.5000\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.6369 - acc: 0.6613 - val_loss: 0.7158 - val_acc: 0.5150\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.6365 - acc: 0.6588 - val_loss: 0.7105 - val_acc: 0.4950\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.6360 - acc: 0.6550 - val_loss: 0.7104 - val_acc: 0.4950\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.6354 - acc: 0.6525 - val_loss: 0.7126 - val_acc: 0.5000\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6346 - acc: 0.6625 - val_loss: 0.7071 - val_acc: 0.4950\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 162us/step - loss: 0.6341 - acc: 0.6613 - val_loss: 0.7090 - val_acc: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1befde829e8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100)) # X data\n",
    "labels = np.random.randint(2, size=(1000, 1)) # y data\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, validation_split=0.20, epochs=100, batch_size=32)\n",
    "\n",
    "#Note that you can also use train_test_split() with , validation_data=(X_test,y_test) argument from Keras in same manner.\n",
    "##Split data first and then simply train on training data and add test data to this argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.4178 - acc: 0.0960\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 2.3972 - acc: 0.0930\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 2.3818 - acc: 0.0940\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 2.3699 - acc: 0.0970\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 2.3607 - acc: 0.0900\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 2.3534 - acc: 0.0910\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 2.3475 - acc: 0.0860\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 2.3426 - acc: 0.0940\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 2.3385 - acc: 0.0970\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.3352 - acc: 0.0970\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.3321 - acc: 0.0960\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 2.3296 - acc: 0.0960\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.3294 - acc: 0.140 - 0s 56us/step - loss: 2.3274 - acc: 0.0980\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 2.3254 - acc: 0.0950\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 2.3237 - acc: 0.0940\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.3222 - acc: 0.0970\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 2.3208 - acc: 0.0990\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 2.3197 - acc: 0.0990\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 2.3186 - acc: 0.0980\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 2.3176 - acc: 0.1030\n",
      "100/100 [==============================] - 2s 16ms/step\n",
      "[2.3609211444854736, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "# for multiple categories you  need to one hot encode y using to_categorical()\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(32, activation='relu', input_dim=20))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01)  # define a learning rate for optimization\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128) # extract loss and accuracy from test data evaluation\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "[7 6 8 7 5 1 7 6 6 1 6 1 6 4 1 7 4 8 7 1 7 1 4 6 1 1 1 8 1 1 7 1 4 7 1 7 1\n",
      " 6 8 7 0 1 1 8 1 8 7 1 7 1 7 5 1 3 1 6 7 1 8 1 6 7 7 7 1 8 6 4 5 7 4 8 4 0\n",
      " 5 1 1 8 6 4 4 8 1 1 8 7 4 6 0 7 1 1 7 6 1 1 6 1 7 1]\n",
      "[[0.07374199 0.12213745 0.07685741 0.10939735 0.08792505 0.09080634\n",
      "  0.10589595 0.12896806 0.11801413 0.08625622]\n",
      " [0.09849285 0.11407992 0.09522335 0.090311   0.09342678 0.09299271\n",
      "  0.11455514 0.11118037 0.10531382 0.08442403]\n",
      " [0.09952311 0.10705225 0.07021143 0.10744977 0.11171241 0.10613842\n",
      "  0.10430415 0.07836949 0.11576303 0.09947591]\n",
      " [0.07571634 0.10092724 0.05257684 0.10454383 0.10507186 0.11376609\n",
      "  0.12094142 0.1378554  0.10137254 0.08722848]\n",
      " [0.10157811 0.11081247 0.07302648 0.09588359 0.10053616 0.12354809\n",
      "  0.10258005 0.07706122 0.11739568 0.09757815]\n",
      " [0.07336328 0.14790872 0.08815695 0.09500729 0.09210476 0.07790933\n",
      "  0.11436201 0.11360762 0.1370281  0.0605519 ]\n",
      " [0.08506425 0.10187812 0.07374813 0.09658505 0.11567329 0.09795389\n",
      "  0.10892223 0.13741407 0.10869998 0.07406102]\n",
      " [0.08546808 0.12273864 0.08845481 0.0770817  0.07982516 0.10379431\n",
      "  0.14347094 0.09385605 0.13394137 0.07136896]\n",
      " [0.1019741  0.09998255 0.06964649 0.07506222 0.12070619 0.10238451\n",
      "  0.16019548 0.10494397 0.07898919 0.08611533]\n",
      " [0.08863938 0.17653644 0.07328306 0.104295   0.08777759 0.10465623\n",
      "  0.09337437 0.09248257 0.11635328 0.06260198]\n",
      " [0.07934623 0.11883202 0.08660325 0.11875147 0.10492913 0.07111418\n",
      "  0.12363947 0.1230917  0.08603396 0.08765855]\n",
      " [0.09478017 0.12201044 0.07992662 0.09937486 0.10856068 0.09300853\n",
      "  0.09901389 0.1190128  0.10104011 0.08327188]\n",
      " [0.11814824 0.11221689 0.07437623 0.09054904 0.1299639  0.07840308\n",
      "  0.1357333  0.08638844 0.09100403 0.08321682]\n",
      " [0.09841954 0.10815694 0.08357878 0.08290288 0.1325588  0.08926088\n",
      "  0.11289747 0.10919083 0.09627096 0.0867629 ]\n",
      " [0.06435369 0.14659177 0.10435896 0.10502328 0.08926225 0.08407266\n",
      "  0.11700731 0.12189463 0.0912346  0.07620076]\n",
      " [0.09502335 0.12113696 0.07838784 0.09859187 0.0847035  0.08313468\n",
      "  0.10240219 0.13466012 0.10201456 0.0999449 ]\n",
      " [0.11593822 0.10233115 0.09360941 0.07567774 0.11978251 0.08206641\n",
      "  0.10618491 0.10394257 0.10382131 0.09664585]\n",
      " [0.13884692 0.11980177 0.08699416 0.07300048 0.12301436 0.07861298\n",
      "  0.10862663 0.07251279 0.13903213 0.05955786]\n",
      " [0.08299223 0.12235314 0.07708447 0.10198871 0.09306418 0.09704368\n",
      "  0.10538491 0.1332409  0.08491988 0.10192795]\n",
      " [0.10017081 0.13304044 0.09138013 0.0761677  0.10857794 0.07347616\n",
      "  0.13098267 0.08750667 0.11904982 0.0796477 ]\n",
      " [0.07807321 0.12625022 0.0719845  0.12263425 0.10292816 0.08866023\n",
      "  0.09088738 0.13627602 0.08280366 0.09950244]\n",
      " [0.11257619 0.12726608 0.12267332 0.08477061 0.0940834  0.07166087\n",
      "  0.09939687 0.08647135 0.12299415 0.07810713]\n",
      " [0.09704863 0.09862924 0.06973348 0.11506528 0.13326907 0.0986834\n",
      "  0.10470881 0.09945731 0.08373161 0.09967314]\n",
      " [0.08480863 0.12990762 0.10003577 0.08147489 0.09924059 0.07940969\n",
      "  0.17138821 0.09466758 0.08920964 0.06985743]\n",
      " [0.09920754 0.12221261 0.08818458 0.11225031 0.09212917 0.08392434\n",
      "  0.08369034 0.11726319 0.11028778 0.09085009]\n",
      " [0.08213367 0.13833629 0.07355174 0.10724247 0.07950015 0.1201778\n",
      "  0.10190241 0.12082697 0.08956742 0.08676112]\n",
      " [0.07763463 0.14013584 0.11175106 0.09727145 0.08949915 0.08416907\n",
      "  0.10854501 0.09447629 0.11553655 0.08098086]\n",
      " [0.08090814 0.1171161  0.07582486 0.08903029 0.09893867 0.08382123\n",
      "  0.11355417 0.09234925 0.17371547 0.07474189]\n",
      " [0.07479004 0.12522048 0.07217988 0.1140738  0.09892349 0.10265888\n",
      "  0.12134566 0.1202613  0.09119583 0.07935069]\n",
      " [0.07322454 0.14239502 0.07685846 0.1213858  0.09339164 0.07924024\n",
      "  0.09389568 0.10687792 0.14106028 0.07167041]\n",
      " [0.09923209 0.10956033 0.0743378  0.0941882  0.11476889 0.07600118\n",
      "  0.11646654 0.11839305 0.10638065 0.09067123]\n",
      " [0.08421704 0.16177283 0.08867665 0.11252017 0.08476812 0.06831901\n",
      "  0.11931533 0.12102813 0.08149862 0.07788411]\n",
      " [0.12363685 0.09635253 0.08698562 0.08328296 0.13073954 0.06745388\n",
      "  0.10239753 0.1067341  0.10652783 0.09588913]\n",
      " [0.08313001 0.12145741 0.06152326 0.12309904 0.10237152 0.08934734\n",
      "  0.08385865 0.13898982 0.12562518 0.07059779]\n",
      " [0.07875729 0.13721475 0.10406842 0.07958455 0.12827219 0.07452509\n",
      "  0.13317688 0.0910513  0.11647877 0.05687075]\n",
      " [0.07743659 0.1240179  0.08134104 0.12605248 0.10087367 0.06379031\n",
      "  0.08652472 0.15385287 0.09305664 0.09305388]\n",
      " [0.0749938  0.15026489 0.09249906 0.08822731 0.08727235 0.0966805\n",
      "  0.13593414 0.09920566 0.10632218 0.06860004]\n",
      " [0.0704444  0.12480419 0.0924935  0.09906999 0.10393562 0.08767828\n",
      "  0.13172983 0.09647182 0.11355854 0.07981381]\n",
      " [0.08203769 0.12572122 0.09216569 0.08161981 0.09670556 0.08670865\n",
      "  0.13139606 0.08738441 0.14694317 0.06931778]\n",
      " [0.0772745  0.12126451 0.09227324 0.09840681 0.10121362 0.09408348\n",
      "  0.11450993 0.12712894 0.08189717 0.09194782]\n",
      " [0.1311952  0.1022606  0.0874342  0.07771561 0.12891011 0.09910406\n",
      "  0.11019831 0.07308941 0.11079857 0.07929398]\n",
      " [0.08960612 0.1555953  0.1057269  0.09103235 0.07386599 0.1005756\n",
      "  0.11561258 0.08800904 0.11440077 0.06557533]\n",
      " [0.08045817 0.12979491 0.07513586 0.11154459 0.07913118 0.10636736\n",
      "  0.09004264 0.11730389 0.11319499 0.09702638]\n",
      " [0.08916727 0.10085999 0.08085018 0.08704176 0.09754805 0.08768452\n",
      "  0.11892145 0.10581739 0.14165463 0.09045473]\n",
      " [0.10911074 0.12628445 0.08209423 0.09756391 0.11591356 0.08766478\n",
      "  0.10896092 0.07261316 0.12442706 0.07536723]\n",
      " [0.10299661 0.11078191 0.07922128 0.0941447  0.0956528  0.08279379\n",
      "  0.10073937 0.11384906 0.12252117 0.09729931]\n",
      " [0.08890199 0.10014121 0.05343058 0.09864297 0.13593298 0.07691119\n",
      "  0.13605483 0.14106956 0.06880874 0.10010592]\n",
      " [0.0692777  0.134105   0.07831038 0.11260777 0.10197909 0.10807625\n",
      "  0.11843835 0.09551578 0.09091899 0.09077068]\n",
      " [0.08458465 0.10569847 0.07782968 0.09907515 0.106787   0.08740252\n",
      "  0.11299949 0.12513916 0.09072427 0.10975955]\n",
      " [0.09561363 0.1365224  0.08437015 0.09886058 0.10816551 0.09101277\n",
      "  0.09678712 0.11033645 0.11356639 0.0647649 ]\n",
      " [0.07399338 0.12120821 0.07182483 0.13984667 0.08678326 0.08007437\n",
      "  0.06598642 0.1732908  0.10060202 0.08639002]\n",
      " [0.11205343 0.12152144 0.06619898 0.09845546 0.09138534 0.12877876\n",
      "  0.08279093 0.0992199  0.09719522 0.10240054]\n",
      " [0.07721421 0.11997925 0.07189546 0.11709329 0.11317804 0.10660674\n",
      "  0.10269397 0.11515594 0.11308644 0.06309664]\n",
      " [0.09512584 0.11756729 0.07805791 0.12063976 0.10967657 0.08448365\n",
      "  0.08613553 0.11965147 0.10133477 0.08732727]\n",
      " [0.08211721 0.12432487 0.08307423 0.11221547 0.10770688 0.09491454\n",
      "  0.09818038 0.1217721  0.1069931  0.06870127]\n",
      " [0.09681442 0.12093914 0.07365351 0.08507758 0.09640356 0.09305706\n",
      "  0.12466094 0.09506331 0.12322949 0.0911011 ]\n",
      " [0.09412976 0.10475124 0.06631187 0.11161234 0.10370126 0.10010496\n",
      "  0.11639689 0.12806046 0.09735798 0.07757315]\n",
      " [0.08900452 0.12690328 0.10741053 0.09903042 0.08743782 0.09464297\n",
      "  0.10462668 0.09887022 0.10122615 0.09084753]\n",
      " [0.09029452 0.12325796 0.0904306  0.10326292 0.11754927 0.06668007\n",
      "  0.09625306 0.11111175 0.12678903 0.07437083]\n",
      " [0.08859267 0.14142707 0.09088641 0.09649529 0.09548692 0.0898143\n",
      "  0.11377792 0.10544525 0.0985264  0.07954782]\n",
      " [0.07520614 0.11905082 0.05613251 0.10810229 0.10311227 0.10520169\n",
      "  0.14137025 0.12541024 0.09516791 0.0712458 ]\n",
      " [0.10620801 0.0983346  0.08623815 0.08589789 0.09332595 0.09788019\n",
      "  0.10856951 0.1233202  0.09331675 0.10690876]\n",
      " [0.08940033 0.12457014 0.08314034 0.10180292 0.10431441 0.07986531\n",
      "  0.09427223 0.13820742 0.11104282 0.07338402]\n",
      " [0.09163973 0.10608718 0.06257179 0.11302532 0.10625883 0.08494221\n",
      "  0.09128446 0.15219004 0.09025954 0.10174095]\n",
      " [0.0856132  0.14266692 0.07777037 0.11083794 0.08778928 0.0862337\n",
      "  0.1173432  0.08470401 0.131783   0.0752584 ]\n",
      " [0.08401116 0.11009464 0.07865597 0.0983912  0.0950636  0.07567188\n",
      "  0.10699575 0.11085958 0.14702539 0.09323081]\n",
      " [0.09211819 0.10822204 0.10255624 0.09026773 0.09771128 0.10726049\n",
      "  0.12541781 0.09128451 0.0972048  0.08795688]\n",
      " [0.09263107 0.10276494 0.08280853 0.08932346 0.12565051 0.08984813\n",
      "  0.11203534 0.11811475 0.09666082 0.09016234]\n",
      " [0.09182251 0.10430646 0.07596042 0.10040713 0.11552257 0.12158563\n",
      "  0.12065013 0.06083157 0.11738698 0.09152664]\n",
      " [0.10196459 0.11355045 0.07820401 0.10130483 0.09672035 0.09975633\n",
      "  0.09314218 0.11474321 0.09420438 0.10640961]\n",
      " [0.11673891 0.1272145  0.07838745 0.09158219 0.14069043 0.08182688\n",
      "  0.10740402 0.09086368 0.09349843 0.07179352]\n",
      " [0.09242877 0.09822356 0.06034239 0.10245359 0.1411398  0.07346833\n",
      "  0.12332266 0.07917719 0.15094419 0.07849957]\n",
      " [0.09426959 0.08554675 0.06508473 0.09703572 0.18760571 0.05531767\n",
      "  0.10127976 0.12291777 0.1088374  0.08210484]\n",
      " [0.11647804 0.10714536 0.07856601 0.09973484 0.11403675 0.09280085\n",
      "  0.10529297 0.09503675 0.09625626 0.09465218]\n",
      " [0.11238059 0.11987931 0.08793039 0.10089929 0.09582362 0.12498429\n",
      "  0.0884819  0.08582997 0.09374635 0.09004432]\n",
      " [0.08458074 0.15068674 0.08167606 0.10496657 0.10642607 0.07022724\n",
      "  0.14576608 0.10537787 0.06726667 0.08302595]\n",
      " [0.11203948 0.14875408 0.072732   0.10213074 0.12425814 0.08395091\n",
      "  0.08688793 0.09414613 0.10709613 0.06800441]\n",
      " [0.0716412  0.1317722  0.07912446 0.08496408 0.12145188 0.08439752\n",
      "  0.13365982 0.0913114  0.14554234 0.05613506]\n",
      " [0.09952065 0.09715177 0.07683488 0.08521254 0.11740322 0.07821266\n",
      "  0.11814665 0.11012457 0.11742137 0.0999716 ]\n",
      " [0.10156232 0.12370244 0.078468   0.07575504 0.14171538 0.06128039\n",
      "  0.12108085 0.0997416  0.1232134  0.0734805 ]\n",
      " [0.09470025 0.09385893 0.08584396 0.09908153 0.13344984 0.09361169\n",
      "  0.11700459 0.07240155 0.1138209  0.09622679]\n",
      " [0.08977136 0.10708312 0.08182116 0.09423329 0.10085199 0.08212692\n",
      "  0.10772744 0.08212555 0.16261901 0.09164011]\n",
      " [0.10442139 0.1413348  0.11029445 0.09562652 0.0783421  0.09089371\n",
      "  0.09662221 0.0873946  0.12034623 0.07472399]\n",
      " [0.05318027 0.14268064 0.07695542 0.13101943 0.08337805 0.09585063\n",
      "  0.10363925 0.12852748 0.09714355 0.08762532]\n",
      " [0.08566095 0.12343648 0.06748489 0.09203232 0.08699822 0.11862487\n",
      "  0.10940387 0.0891209  0.13033257 0.0969049 ]\n",
      " [0.10307895 0.0924891  0.07125808 0.08520171 0.11465931 0.08812961\n",
      "  0.12083865 0.12343229 0.09528083 0.1056314 ]\n",
      " [0.12969922 0.11153753 0.09026785 0.0681329  0.1301128  0.08671241\n",
      "  0.11550562 0.07772832 0.11337181 0.07693147]\n",
      " [0.09562356 0.1064461  0.07450107 0.0866401  0.10569424 0.0990876\n",
      "  0.12017351 0.10206581 0.11483468 0.09493323]\n",
      " [0.13153508 0.09919571 0.07709249 0.09776483 0.1142451  0.0785675\n",
      "  0.11533365 0.08125081 0.10930913 0.09570577]\n",
      " [0.08920696 0.10061748 0.06341554 0.09975297 0.11566586 0.08457278\n",
      "  0.11989067 0.13437769 0.10789122 0.08460874]\n",
      " [0.09051792 0.13472956 0.08246928 0.09441413 0.11615041 0.07304736\n",
      "  0.10777795 0.11478066 0.12035878 0.06575383]\n",
      " [0.08037412 0.12738131 0.07167829 0.09620027 0.09658941 0.08263384\n",
      "  0.1260155  0.11000111 0.11648159 0.09264455]\n",
      " [0.09065145 0.10682225 0.07668658 0.093174   0.12600653 0.07500412\n",
      "  0.11099165 0.12700559 0.1030563  0.09060161]\n",
      " [0.08663248 0.12620556 0.09706241 0.08678035 0.13100642 0.06531756\n",
      "  0.13814294 0.09269757 0.11299217 0.06316238]\n",
      " [0.081262   0.1364019  0.06875439 0.12214091 0.11886854 0.08645266\n",
      "  0.08441011 0.12224843 0.10640226 0.07305883]\n",
      " [0.09950535 0.11478578 0.09235    0.1027533  0.09495567 0.09732825\n",
      "  0.09752958 0.11315285 0.11155349 0.0760857 ]\n",
      " [0.07302657 0.12771735 0.09401198 0.08436031 0.07396147 0.10650711\n",
      "  0.14824037 0.11149602 0.09348965 0.08718925]\n",
      " [0.09308423 0.14213227 0.07709063 0.10089965 0.1001175  0.0737852\n",
      "  0.12033603 0.11630291 0.10057741 0.07567412]\n",
      " [0.08876623 0.10255752 0.07908436 0.11551216 0.11160579 0.09327203\n",
      "  0.08336335 0.13637951 0.09202376 0.09743533]\n",
      " [0.09691004 0.13109513 0.09897891 0.09602202 0.09637488 0.0863103\n",
      "  0.10209699 0.1053349  0.11136297 0.07551384]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prediction from keras classification model\n",
    "print(x_test.shape)\n",
    "\n",
    "# for predicted probabilities and labels\n",
    "ypreds = model.predict_classes(x_test)\n",
    "print(ypreds)\n",
    "\n",
    "# for predicted probabilities\n",
    "ypreds = model.predict_proba(x_test)\n",
    "print(ypreds) #gives prediction of each category, largest is selected for predict_classes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction from keras regression model\n",
    "\n",
    "# for predicted probabilities and labels\n",
    "ypreds = model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict keras model with sklearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332, 5)\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# Use KerasRegressor for regression model tuning\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pimas diabetes dataset\n",
    "dataset = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Pima.te.csv\", delimiter=\",\")\n",
    "\n",
    "X = dataset.iloc[:,1:6]\n",
    "Y = dataset.iloc[:,7]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple example\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=5, activation='relu'))\n",
    "\tmodel.add(Dense(8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, verbose=0) # epochs arg is built in to Scikit learn's... \n",
    "                                                                      # KerasClassifier\n",
    "\n",
    "# Building a simple search grid that adjusts epochs\n",
    "param_grid = dict(epochs=[10,20,30])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.123494 using {'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "# grid_result.cv_results_ for full results file\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.123494 using {'learn_rate': 0.001}\n",
      "0.123494 (0.004543) with: {'learn_rate': 0.001}\n",
      "0.123494 (0.004543) with: {'learn_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create function that builds model\n",
    "# Function to create model, required for KerasClassifier\n",
    "\n",
    "#In order to tune parameters native to keras, add them as arguments to your create_model function\n",
    "\n",
    "def create_model(learn_rate=0.01):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=5, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\toptimizer = SGD(lr=learn_rate)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "#call model function in KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate) # set dictionary using function this time\n",
    "\n",
    "#Using n_jobs=-1 to parallelize across available processors to speed it up\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you try.  Can you fit a neural network model to the Iris dataset?  Run models that change the structure of the network (i.e.-hidden layers and activations).  Try to improve your validation accuracy as much as possible.\n",
    "\n",
    "Data can be imported via the following link:\n",
    "\n",
    "http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0          setosa\n",
       "10         setosa\n",
       "20         setosa\n",
       "30         setosa\n",
       "40         setosa\n",
       "50     versicolor\n",
       "60     versicolor\n",
       "70     versicolor\n",
       "80     versicolor\n",
       "90     versicolor\n",
       "100     virginica\n",
       "110     virginica\n",
       "120     virginica\n",
       "130     virginica\n",
       "140     virginica\n",
       "Name: Species, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# target = InMichelin, whether or not a restaurant is in the Michelin guide\n",
    "data = pd.read_csv(\"http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\" , encoding=\"latin_1\")\n",
    "\n",
    "\n",
    "#update data to set up for train test split\n",
    "data = data.iloc[:,1:]\n",
    "y = data['Species']\n",
    "X = data.loc[:, data.columns != 'Species']\n",
    "\n",
    "display(data.head())\n",
    "display(X.head())\n",
    "display(y[0::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

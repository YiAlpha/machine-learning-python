{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Models: Ordinary Least square(OlS), Ridge regression, Lasso Regression, (Penalized) Logistic regression, \n",
    "Tree Models: Decision Tree, Ensembles of Decision Trees(Random forests, Gradient boosted regression trees)\n",
    "\n",
    "These models above provide comparison of independent variables(some need scalling before process) through the magnitude of the coefficients or feature importance, so we can use those information to select meaningful covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe the main differences between supervised and unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **supervised learning**, there is known output, and the goal is to predict a certain outcome from new, never-before-seen data. When we train the model, the known output is used to calibrate the model for better prediction on new data.\n",
    "\n",
    "In **Unsupervised learning**, there is no known output, and the goal is to extract knowledge from input data, no expected output to calibrate the unsupervised learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning** is the primary approach that is used by machine learning practitioners. Learn from input/output pairs, supervised learning model can automate decision-making processes by generalizing from known examples, producing reasonable predictions. Supervised learning can be used in real business circumstance: fraud detection, spam detection, medical image recognition.etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning** can extract information from the data, or reduce the dimension of the data for visulization. It's hard to evaluate the Unsupervised learning model since this is no known output. Sometimes, Unsupervised learning can be used to simplify or preprocess the data before Supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA); Manifold Learning(with t-SNE); Clustering (k-Means; Agglomerative)\n",
    "\n",
    "**Principal component analysis** is to find linear combinations of the predictors, which capture the most possible variance for dimension reduction. PCA can be used as preprocess method for supervised learning, and visualization.\n",
    "\n",
    "**Manifold Learning with t-SNE** is to find a two-dimensional representation of the data that preserves the distances between points as best as possible. Manifold Learning with t-SNE mainly used for visualization.\n",
    "\n",
    "**Clustering** is to partition the dataset into groups, where data within group are similar, between groups are different. Clustering is used to learn the pattern of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  What are the main benefits of using Principal Components Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Principal Components Analysis (PCA)**, as a dimension reduction method, creates new predictors (components) that are uncorrelated. Some predictive models prefer predictors to be uncorrelated, PCA meets this constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) PCA ill firstly summarizing predictors that have more variation, latter summarize lower variance predictors. Therefore PCA produce a simplier representation than the raw representation if we specify the number of the components, while caputureing the variance as much as possibile, which is good feature extraction and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural networks, takes some input data, transforms the input data by assign weights and bias, and apply nonlinear function to calculate an intermediate result. The three steps call a layer, the nonlinear function is the activation function, and  a neuron is the basic operational unit. The intermediate result are used as the input for another layer, called hidden layer.\n",
    "\n",
    "1. The first difference between a multilayer perceptron (MLP) network and a convolutional neural network (CNN) model lies in their input space : MLP’s input is features while CNN’s input is source data such as image, video, speech, etc. For example, in image classification, MLP use all pixels, CNN take advantage of  architecture of images (width, height, and color channels)\n",
    "2. In MLP, the input layer, hidden layers and output layer and fully-connected.  There are a weight and a bias parameter associated with every neuron. CNN use groups of neurons  as 1 unit in the next layer, within the group, neurons share the same weights, which reduce the computational complexity. The next layer is called convolutional layer.\n",
    "3. MLP do not cut the number of neurons. After the convolutional layer, CNN pool neurons within a group into a single unit (by taking the max, average or other selection criteria), called pooling, which reduce the dimension of the input for next layer.\n",
    "\n",
    "In essence, CNN extracts features from the source data via the convolutional layers, and the pooling layers, and making decision through subsequent fully-connect layers, while MLP can not automatically extract features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write the keras code for a multilayer perceptron neural network with the following structure:\n",
    "\n",
    "- Three hidden layers.  \n",
    "    50 hidden units in the first hidden layer, \n",
    "    100 in the second, \n",
    "    and 150 in the third.  \n",
    "\n",
    "- Activate all hidden layers with relu.  \n",
    "\n",
    "- The output layer should be built to classify to five categories.  \n",
    "\n",
    "- Further, your optimization technique should be stochastic gradient descent.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=50, activation='relu', input_dim=input_dim))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=150, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Write the keras code for a multilayer perceptron neural network with the following structure: \n",
    "\n",
    "- Two hidden layers.  \n",
    "    75 hidden units in the first hidden layer and \n",
    "    150 in the second.  \n",
    "- Activate all hidden layers with relu. \n",
    "- The output layer should be built to classify a binary dependent variable.  \n",
    "- Further, your optimization technique should be stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=75, activation='relu', input_dim=input_dim))\n",
    "model.add(Dense(units=150, activation='relu')))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write the keras code for a convolutional neural network with the following structure:\n",
    "\n",
    "- Two convolutional layers.  \n",
    "  16 filters in the first hidden layer and \n",
    "  28 in the second.  \n",
    "\n",
    "- Activate all convolutional layers with relu.  \n",
    "- Use max pooling after each convolutional layer with a 2 by 2 filter.  \n",
    "- The output layer should be built to classify to ten categories.  \n",
    "\n",
    "- Further, your optimization technique should be stochastic gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(28, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))   \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  Write the keras code for a convolutional neural network with the following structure: \n",
    "\n",
    "- Two convolutional layers.  \n",
    "    32 filters in the first hidden layer and \n",
    "    32 in the second.  \n",
    "- Activate all convolutional layers with relu.  \n",
    "- Use max pooling after each convolutional layer with a 2 by 2 filter.  \n",
    "- Add two fully connected layers with 128 hidden units in each layer and relu activations.  \n",
    "- The output layer should be built to classify to six categories.  \n",
    "- Further, your optimization technique should be stochastic gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

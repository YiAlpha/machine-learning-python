{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "HW2 reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import some example data\n",
    "\n",
    "import pandas as pd\n",
    "# target = InMichelin, whether or not a restaurant is in the Michelin guide\n",
    "# do not forget the encoding \n",
    "data = pd.read_csv(\"http://gattonweb.uky.edu/sheather/book/docs/datasets/MichelinNY.csv\" , encoding=\"latin_1\")\n",
    "data.head()\n",
    "\n",
    "#update data to set up for train test split\n",
    "data = data.loc[:, data.columns != 'Restaurant Name']\n",
    "y = data['InMichelin']\n",
    "X = data.loc[:, data.columns != 'InMichelin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.3810703   0.07415961 -0.1569253   0.0819146 ]]\n",
      "Training set score: 0.797\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "#Set up training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# random_state like set.seed in R\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42) \n",
    "\n",
    "#Note: random_state ensures same data will be generated for example each time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Note: logistic regression in sklearn is preset to be a regularization model with C=100).\n",
    "#If you make C really high the model effectively becomes a logistic regression model...\n",
    "\n",
    "# C is the tuning parameter like alpha in Reidge and Lasso regression\n",
    "# Reguilization\n",
    "\n",
    "# set C high to get normal logistic regression\n",
    "logreg = LogisticRegression(C=1e90).fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+90, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg\n",
    "\n",
    "#Use ?LogisticRegression() for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in statsmodels package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>InMichelin</td>    <th>  No. Observations:  </th>  <td>   123</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   118</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     4</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -57.266</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Thu, 04 Oct 2018</td> <th>  Deviance:          </th> <td>  114.53</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>17:00:58</td>     <th>  Pearson chi2:      </th>  <td>  254.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>6</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  -10.6490</td> <td>    2.588</td> <td>   -4.115</td> <td> 0.000</td> <td>  -15.722</td> <td>   -5.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Food</th>    <td>    0.3818</td> <td>    0.148</td> <td>    2.572</td> <td> 0.010</td> <td>    0.091</td> <td>    0.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Decor</th>   <td>    0.0743</td> <td>    0.103</td> <td>    0.720</td> <td> 0.471</td> <td>   -0.128</td> <td>    0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Service</th> <td>   -0.1569</td> <td>    0.147</td> <td>   -1.070</td> <td> 0.285</td> <td>   -0.444</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Price</th>   <td>    0.0819</td> <td>    0.036</td> <td>    2.269</td> <td> 0.023</td> <td>    0.011</td> <td>    0.153</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:             InMichelin   No. Observations:                  123\n",
       "Model:                            GLM   Df Residuals:                      118\n",
       "Model Family:                Binomial   Df Model:                            4\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -57.266\n",
       "Date:                Thu, 04 Oct 2018   Deviance:                       114.53\n",
       "Time:                        17:00:58   Pearson chi2:                     254.\n",
       "No. Iterations:                     6   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -10.6490      2.588     -4.115      0.000     -15.722      -5.576\n",
       "Food           0.3818      0.148      2.572      0.010       0.091       0.673\n",
       "Decor          0.0743      0.103      0.720      0.471      -0.128       0.277\n",
       "Service       -0.1569      0.147     -1.070      0.285      -0.444       0.131\n",
       "Price          0.0819      0.036      2.269      0.023       0.011       0.153\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same using different package like lm() in R\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# add constant to matrix\n",
    "X_train_new = sm.add_constant(X_train)\n",
    "\n",
    "# Generized linear model using binomial model,\n",
    "model = sm.GLM(y_train, X_train_new, family=sm.families.Binomial()).fit()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with constraints on size of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.35927528  0.06667776 -0.16192467  0.08317683]]\n",
      "Training set score: 0.797\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Smaller C will constrain Betas more.  It's a tuning parameter we can find using gridsearch.\n",
    "# Smaller C = Stronger constraints\n",
    "\n",
    "#C=100, compare coefs to regular model above.\n",
    "logreg = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.08104081 -0.04500912 -0.25388726  0.11700635]]\n",
      "Training set score: 0.756\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#C=1, compare coefs to above models.\n",
    "logreg = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.08104081 -0.04500912 -0.25388726  0.11700635]]\n",
      "Training set score: 0.756\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#C=.01, compare coefs to above models.\n",
    "\n",
    "#Does the model's prediction power get better or worse??\n",
    "\n",
    "logreg = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass models (Multinomial model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "iris\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(iris.feature_names )# X variable names\n",
    "print(X[0:5]) # first five rows of data\n",
    "\n",
    "print(iris.target_names) #target categories\n",
    "print(np.unique(y)) #target values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note the argument changes to LogisticRegression()\n",
    "# change from binomial to multinomial \n",
    "logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\").fit(X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(logreg.predict(X)) #uses softmax function to predict new X data, but I am being lazy and using X data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
